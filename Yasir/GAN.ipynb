{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.layers.merge import _Merge, concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from matplotlib import colors as mcol\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from functools import partial\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    ActivityRegularization,\n",
    "    Lambda,\n",
    "    Concatenate,\n",
    "    Permute,\n",
    "    Convolution1D,\n",
    "    MaxPooling1D,\n",
    "    AveragePooling1D,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Setting up Environment varialbes\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "IMAGE_DIR_PATH = \"gallery/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.randn(10000, 4)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD_loss(x, y):\n",
    "    \n",
    "    sigma = 0.1\n",
    "    x1 = x[:HALF_BATCH, :]\n",
    "    x2 = x[HALF_BATCH:, :]\n",
    "    y1 = y[:HALF_BATCH, :]\n",
    "    y2 = y[HALF_BATCH:, :]  \n",
    "    \n",
    "    x1_x2 = K.sum(K.exp(sigma/((x1-x2)*(x1-x2)+sigma)))/HALF_BATCH\n",
    "    y1_y2 = K.sum(K.exp(sigma/((y1-y2)*(y1-y2)+sigma)))/HALF_BATCH\n",
    "    x_y = K.sum(K.exp(sigma/((x-y)*(x-y)+sigma)))/BATCH_SIZE\n",
    "    \n",
    "    return (x1_x2 + y1_y2 - 2*x_y)*(x1_x2 + y1_y2 - 2*x_y) \n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def gradient_penalty_loss(y_true, y_pred,\n",
    "                          averaged_samples, gradient_penalty_weight):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradients_sqr_sum = K.sum(\n",
    "        gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape))\n",
    "    )\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Takes a randomly-weighted average of two tensors. In geometric terms,\n",
    "    this outputs a random point on the line between each pair of input points.\n",
    "    \"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        weights = K.random_uniform((BATCH_SIZE, 1))\n",
    "        return (weights * inputs[0]) + ((1 - weights) * inputs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_feature(x):\n",
    "    \n",
    "#     px = x[:, 0:1]\n",
    "#     py = x[:, 1:2]\n",
    "#     pz = x[:, 2:3]\n",
    "#     energy = K.sqrt( (px * px) + (py * py) + (pz * pz) )\n",
    "\n",
    "#     return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    visible = Input(shape=(100,))\n",
    "    hidden1 = Dense(512)(visible)\n",
    "    LR = LeakyReLU()(hidden1)\n",
    "    hidden2 = Dense(512)(LR)\n",
    "    LR = LeakyReLU()(hidden2)\n",
    "    hidden3 = Dense(512)(LR)\n",
    "    LR = LeakyReLU(alpha=0.2)(hidden3)\n",
    "    hidden4 = Dense(512)(LR)\n",
    "    LR = LeakyReLU()(hidden4)\n",
    "    hidden5 = Dense(512)(LR)\n",
    "    LR = LeakyReLU()(hidden5)\n",
    "    output = Dense(4)(LR)\n",
    "#     energy = Lambda(pt_mul)(output)\n",
    "#     outputmerge = concatenate([output, energy])\n",
    "    generator = Model(inputs=visible, outputs=[output])\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 1,104,388\n",
      "Trainable params: 1,104,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "    visible = Input(shape=(4,))\n",
    "    hidden1 = Dense(512)(visible)\n",
    "    LR = LeakyReLU()(hidden1)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    hidden2 = Dense(512)(DR)\n",
    "    LR = LeakyReLU(alpha=0.2)(hidden2)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    hidden3 = Dense(512)(DR)\n",
    "    LR = LeakyReLU()(hidden3)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    hidden4 = Dense(512)(DR)\n",
    "    LR = LeakyReLU()(hidden4)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    hidden5 = Dense(512)(DR)\n",
    "    LR = LeakyReLU()(hidden5)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    output = Dense(1)(DR)\n",
    "    discriminator = Model(inputs=[visible], outputs=output)\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,053,697\n",
      "Trainable params: 1,053,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HALF_BATCH and FULL_BATCH SIZES\n",
    "HALF_BATCH = 1000\n",
    "BATCH_SIZE = HALF_BATCH * 2\n",
    "\n",
    "# The training ratio is the number of discriminator updates\n",
    "TRAINING_RATIO = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "\n",
    "\n",
    "def make_MMD():\n",
    "    visible = Input(shape=(4,))\n",
    "    MMD = Model(inputs=visible, output=visible)\n",
    "\n",
    "    return MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_FAT_GAN(data):\n",
    "    generator = make_generator()\n",
    "    discriminator = make_discriminator()\n",
    "    MMD = make_MMD()\n",
    "\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable = False\n",
    "    discriminator.trainable = False\n",
    "    generator_input = Input(shape=(100,))\n",
    "    generator_layers = generator(generator_input)\n",
    "    discriminator_layers_for_generator = discriminator(generator_layers)\n",
    "    MMD_Layers_for_generator = MMD(generator_layers)\n",
    "    generator_model = Model(\n",
    "        inputs=generator_input,\n",
    "        outputs=[discriminator_layers_for_generator, MMD_Layers_for_generator],\n",
    "    )\n",
    "    # We use the Adam paramaters from Gulrajani et al.\n",
    "    generator_model.compile(\n",
    "        optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\n",
    "        loss=[wasserstein_loss, MMD_loss],\n",
    "    )\n",
    "    generator_model.summary()\n",
    "\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable = True\n",
    "    for layer in generator.layers:\n",
    "        layer.trainable = False\n",
    "    discriminator.trainable = True\n",
    "    generator.trainable = False\n",
    "\n",
    "    real_samples = Input(shape=data.shape[1:])\n",
    "    generator_input_for_discriminator = Input(shape=(100,))\n",
    "    generated_samples_for_discriminator = generator(generator_input_for_discriminator)\n",
    "    discriminator_output_from_generator = discriminator(generated_samples_for_discriminator)\n",
    "    discriminator_output_from_real_samples = discriminator(real_samples)\n",
    "\n",
    "    # We also need to generate weighted-averages of real and generated samples,\n",
    "    # to use for the gradient norm penalty.\n",
    "    averaged_samples = RandomWeightedAverage()(\n",
    "        [real_samples, generated_samples_for_discriminator]\n",
    "    )\n",
    "\n",
    "    # We then run these samples through the discriminator as well.\n",
    "    # Note that we never really use the discriminator output for these samples,\n",
    "    # we're only running them to get the gradient norm for the gradient\n",
    "    # penalty loss.\n",
    "    averaged_samples_out = discriminator(averaged_samples)\n",
    "\n",
    "    # The gradient penalty loss function requires the input averaged\n",
    "    # samples to get gradients. However, Keras loss functions can only have\n",
    "    # two arguments, y_true and y_pred. We get around this by making\n",
    "    # a partial() of the function with the averaged samples here.\n",
    "    partial_gp_loss = partial(\n",
    "        gradient_penalty_loss,\n",
    "        averaged_samples=averaged_samples,\n",
    "        gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT,\n",
    "    )\n",
    "    # Functions need names or Keras will throw an error\n",
    "    partial_gp_loss.__name__ = \"gradient_penalty\"\n",
    "\n",
    "    # If we don't concatenate the real and generated samples, however,\n",
    "    # we get three outputs: One of the generated samples, one of the real\n",
    "    # samples, and one of the averaged samples, all of size\n",
    "    # BATCH_SIZE. This works neatly!\n",
    "    discriminator_model = Model(\n",
    "        inputs=[real_samples, generator_input_for_discriminator],\n",
    "        outputs=[\n",
    "            discriminator_output_from_real_samples,\n",
    "            discriminator_output_from_generator,\n",
    "            averaged_samples_out,\n",
    "        ],\n",
    "    )\n",
    "    # We use the Adam paramaters from Gulrajani et al. We use the Wasserstein\n",
    "    # loss for both the real and generated samples, and the gradient penalty\n",
    "    # loss for the averaged samples\n",
    "    discriminator_model.compile(\n",
    "        optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\n",
    "        loss=[wasserstein_loss, wasserstein_loss, partial_gp_loss],\n",
    "    )\n",
    "    discriminator_model.summary()\n",
    "\n",
    "    # We make three label vectors for training. positive_y is the label\n",
    "    # vector for real samples, with value 1. negative_y is the label vector\n",
    "    # for generated samples, with value -1. The dummy_y vector is passed to the\n",
    "    # gradient_penalty loss function and is not used.\n",
    "    positive_y = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "    negative_y = -positive_y\n",
    "    dummy_y = np.zeros((BATCH_SIZE, 1), dtype=np.float32)\n",
    "\n",
    "    # Training FAT-GAN for 200,000 epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "        np.random.shuffle(data)\n",
    "        discriminator_loss = []\n",
    "        generator_loss = []\n",
    "        minibatches_size = BATCH_SIZE * TRAINING_RATIO\n",
    "        for i in range(int(data.shape[0] // (BATCH_SIZE * TRAINING_RATIO))):\n",
    "            discriminator_minibatches = data[\n",
    "                i * minibatches_size: (i + 1) * minibatches_size\n",
    "            ]\n",
    "\n",
    "            noise = np.random.normal(0, 1, [BATCH_SIZE * TRAINING_RATIO, 100])\n",
    "\n",
    "            for j in range(TRAINING_RATIO):\n",
    "                image_batch = discriminator_minibatches[\n",
    "                    j * BATCH_SIZE: (j + 1) * BATCH_SIZE\n",
    "                ]\n",
    "                noise_batch = noise[j * BATCH_SIZE: (j + 1) * BATCH_SIZE]\n",
    "\n",
    "                discriminator_loss.append(\n",
    "                    discriminator_model.train_on_batch(\n",
    "                        [image_batch, noise_batch], [positive_y, negative_y, dummy_y]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            noise = np.random.normal(0, 1, [BATCH_SIZE, 100])\n",
    "            generator_loss.append(\n",
    "                generator_model.train_on_batch(noise, [positive_y, image_batch])\n",
    "            )\n",
    "        print(epoch, generator_loss)\n",
    "\n",
    "        # save every 1000 epochs\n",
    "        if epoch % 1000 == 0:\n",
    "            print(epoch)\n",
    "            SAMPLE_SIZE = data.shape[0]\n",
    "            noise = np.random.normal(0, 1, [SAMPLE_SIZE, 100])\n",
    "            results = generator.predict(noise)\n",
    "#             generator.save_weights( IMAGE_DIR_PATH + \"generator\" + str(epoch // 100).zfill(5) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"in...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 4)            1104388     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Model)                 (None, 1)            1053697     model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Model)                 (None, 4)            0           model_3[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,158,085\n",
      "Trainable params: 1,104,388\n",
      "Non-trainable params: 1,053,697\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 4)            1104388     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "random_weighted_average_1 (Rand (None, 4)            0           input_7[0][0]                    \n",
      "                                                                 model_3[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Model)                 (None, 1)            1053697     model_3[2][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 random_weighted_average_1[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 2,158,085\n",
      "Trainable params: 1,053,697\n",
      "Non-trainable params: 1,104,388\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5f91d99f5791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_FAT_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-c87fe3556bc3>\u001b[0m in \u001b[0;36mtrain_FAT_GAN\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Training FAT-GAN for 200,000 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mdiscriminator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "#train_FAT_GAN(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
