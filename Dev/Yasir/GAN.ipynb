{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers.merge import _Merge, concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from matplotlib import colors as mcol\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from matplotlib.colors import LogNorm \n",
    "from functools import partial\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    ActivityRegularization,\n",
    "    Lambda,\n",
    "    Concatenate,\n",
    "    Permute,\n",
    "    Convolution1D,\n",
    "    MaxPooling1D,\n",
    "    AveragePooling1D,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Setting up Environment varialbes\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "IMAGE_DIR_PATH = \"gallery/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.randn(2000,4)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD_loss(x, y):\n",
    "    \"\"\"\n",
    "    This loss function matches the true distribution to the generated ones.\n",
    "    x = true\n",
    "    y = fake\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sigma = 0.1\n",
    "    x1 = x[:HALF_BATCH, :]\n",
    "    x2 = x[HALF_BATCH:, :]\n",
    "    y1 = y[:HALF_BATCH, :]\n",
    "    y2 = y[HALF_BATCH:, :]  \n",
    "    \n",
    "    x1_x2 = K.sum(K.exp(sigma/((x1-x2)*(x1-x2)+sigma)))/HALF_BATCH\n",
    "    y1_y2 = K.sum(K.exp(sigma/((y1-y2)*(y1-y2)+sigma)))/HALF_BATCH\n",
    "    x_y = K.sum(K.exp(sigma/((x-y)*(x-y)+sigma)))/BATCH_SIZE\n",
    "    \n",
    "    return (x1_x2 + y1_y2 - 2*x_y)*(x1_x2 + y1_y2 - 2*x_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred): \n",
    "    \n",
    "    \"\"\"Calculates the Wasserstein loss for a sample batch.\n",
    "    The Wasserstein loss function is very simple to calculate. In a standard GAN, the\n",
    "    discriminator has a sigmoid output, representing the probability that samples are\n",
    "    real or generated. In Wasserstein GANs, however, the output is linear with no\n",
    "    activation function! Instead of being constrained to [0, 1], the discriminator wants\n",
    "    to make the distance between its output for real and generated samples as\n",
    "    large as possible.\n",
    "    The most natural way to achieve this is to label generated samples -1 and real\n",
    "    samples 1, instead of the 0 and 1 used in normal GANs, so that multiplying the\n",
    "    outputs by the labels will give you the loss immediately.\n",
    "    Note that the nature of this loss means that it can be (and frequently will be)\n",
    "    less than 0.\"\"\"\n",
    "    \n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred,\n",
    "                          averaged_samples, gradient_penalty_weight):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradients_sqr_sum = K.sum(\n",
    "        gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape))\n",
    "    )\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Takes a randomly-weighted average of two tensors. In geometric terms,\n",
    "    this outputs a random point on the line between each pair of input points.\n",
    "    \"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        weights = K.random_uniform((BATCH_SIZE, 1))\n",
    "        return (weights * inputs[0]) + ((1 - weights) * inputs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_feature(x):\n",
    "    \n",
    "#     px = x[:, 0:1]\n",
    "#     py = x[:, 1:2]\n",
    "#     pz = x[:, 2:3]\n",
    "#     energy = K.sqrt( (px * px) + (py * py) + (pz * pz) )\n",
    "\n",
    "#     return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    visible = Input(shape=(100,))\n",
    "    hidden1 = Dense(512)(visible)\n",
    "    LR = LeakyReLU()(hidden1)\n",
    "    hidden2 = Dense(512)(LR)\n",
    "    LR = LeakyReLU()(hidden2)\n",
    "    hidden3 = Dense(512)(LR)\n",
    "    LR = LeakyReLU(alpha=0.2)(hidden3)\n",
    "    hidden4 = Dense(512)(LR)\n",
    "    LR = LeakyReLU()(hidden4)\n",
    "    hidden5 = Dense(512)(LR)\n",
    "    LR = LeakyReLU()(hidden5)\n",
    "    output = Dense(4)(LR) # this 4 represents data columns without FAT (extended features)\n",
    "#     energy = Lambda(add_feature)(output)\n",
    "#     outputmerge = concatenate([output, energy])\n",
    "    generator = Model(inputs=visible, outputs=[output])\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 1,104,388\n",
      "Trainable params: 1,104,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "    visible = Input(shape=(4,)) # This is the output vector from the generator/data size\n",
    "    hidden1 = Dense(512)(visible)\n",
    "    LR = LeakyReLU()(hidden1)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    hidden2 = Dense(512)(DR)\n",
    "    LR = LeakyReLU(alpha=0.2)(hidden2)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    hidden3 = Dense(512)(DR)\n",
    "    LR = LeakyReLU()(hidden3)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    hidden4 = Dense(512)(DR)\n",
    "    LR = LeakyReLU()(hidden4)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    hidden5 = Dense(512)(DR)\n",
    "    LR = LeakyReLU()(hidden5)\n",
    "    DR = Dropout(rate=0.1)(LR)\n",
    "    output = Dense(1)(DR)\n",
    "    discriminator = Model(inputs=[visible], outputs=output)\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,053,697\n",
      "Trainable params: 1,053,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HALF_BATCH and FULL_BATCH SIZES\n",
    "HALF_BATCH = 200\n",
    "BATCH_SIZE = HALF_BATCH * 2\n",
    "\n",
    "# The training ratio is the number of discriminator updates\n",
    "TRAINING_RATIO = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "\n",
    "\n",
    "def make_MMD():\n",
    "    visible = Input(shape=(4,))\n",
    "    MMD = Model(inputs=visible, output=visible)\n",
    "\n",
    "    return MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 1,104,388\n",
      "Trainable params: 1,104,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 10\n",
    "def train_FAT_GAN(data):\n",
    "    generator = make_generator()\n",
    "    discriminator = make_discriminator()\n",
    "    MMD = make_MMD()\n",
    "\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable = False\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    generator_input = Input(shape=(100,))\n",
    "    generator_layers = generator(generator_input)\n",
    "    discriminator_layers_for_generator = discriminator(generator_layers)\n",
    "    MMD_Layers_for_generator = MMD(generator_layers)\n",
    "    \n",
    "    generator_model = Model(\n",
    "        inputs=generator_input,\n",
    "        outputs=[discriminator_layers_for_generator, MMD_Layers_for_generator],\n",
    "    )\n",
    "    # We use the Adam paramaters from Gulrajani et al.\n",
    "    generator_model.compile(\n",
    "        optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\n",
    "        loss=[wasserstein_loss, MMD_loss],\n",
    "    )\n",
    "    generator_model.summary()\n",
    "\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable = True\n",
    "    discriminator.trainable = True\n",
    "        \n",
    "        \n",
    "        \n",
    "    for layer in generator.layers:\n",
    "        layer.trainable = False\n",
    "    generator.trainable = False\n",
    "    \n",
    "    \n",
    "    real_samples = Input(shape=data.shape[1:])\n",
    "    generator_input_for_discriminator = Input(shape=(100,))\n",
    "    \n",
    "    generated_samples_for_discriminator = generator(generator_input_for_discriminator)\n",
    "    discriminator_output_from_generator = discriminator(generated_samples_for_discriminator)\n",
    "    discriminator_output_from_real_samples = discriminator(real_samples)\n",
    "\n",
    "    # We also need to generate weighted-averages of real and generated samples,\n",
    "    # to use for the gradient norm penalty.\n",
    "    averaged_samples = RandomWeightedAverage()(\n",
    "        [real_samples, generated_samples_for_discriminator]\n",
    "    )\n",
    "\n",
    "    # We then run these samples through the discriminator as well.\n",
    "    # Note that we never really use the discriminator output for these samples,\n",
    "    # we're only running them to get the gradient norm for the gradient\n",
    "    # penalty loss.\n",
    "    averaged_samples_out = discriminator(averaged_samples)\n",
    "\n",
    "    # The gradient penalty loss function requires the input averaged\n",
    "    # samples to get gradients. However, Keras loss functions can only have\n",
    "    # two arguments, y_true and y_pred. We get around this by making\n",
    "    # a partial() of the function with the averaged samples here.\n",
    "    partial_gp_loss = partial(\n",
    "        gradient_penalty_loss,\n",
    "        averaged_samples=averaged_samples,\n",
    "        gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT,\n",
    "    )\n",
    "    # Functions need names or Keras will throw an error\n",
    "    partial_gp_loss.__name__ = \"gradient_penalty\"\n",
    "\n",
    "    # If we don't concatenate the real and generated samples, however,\n",
    "    # we get three outputs: One of the generated samples, one of the real\n",
    "    # samples, and one of the averaged samples, all of size\n",
    "    # BATCH_SIZE. This works neatly!\n",
    "    discriminator_model = Model(\n",
    "        inputs=[real_samples, generator_input_for_discriminator],\n",
    "        outputs=[\n",
    "            discriminator_output_from_real_samples,\n",
    "            discriminator_output_from_generator,\n",
    "            averaged_samples_out,\n",
    "        ],\n",
    "    )\n",
    "    # We use the Adam paramaters from Gulrajani et al. We use the Wasserstein\n",
    "    # loss for both the real and generated samples, and the gradient penalty\n",
    "    # loss for the averaged samples\n",
    "    discriminator_model.compile(\n",
    "        optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\n",
    "        loss=[wasserstein_loss, wasserstein_loss, partial_gp_loss],\n",
    "    )\n",
    "    discriminator_model.summary()\n",
    "\n",
    "    # We make three label vectors for training. positive_y is the label\n",
    "    # vector for real samples, with value 1. negative_y is the label vector\n",
    "    # for generated samples, with value -1. The dummy_y vector is passed to the\n",
    "    # gradient_penalty loss function and is not used.\n",
    "    positive_y = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "    negative_y= -positive_y\n",
    "    dummy_y = np.zeros((BATCH_SIZE, 1), dtype=np.float32)\n",
    "\n",
    "    # Training FAT-GAN for 200,000 epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "        np.random.shuffle(data)\n",
    "        discriminator_loss = []\n",
    "        generator_loss = []\n",
    "        minibatches_size = BATCH_SIZE * TRAINING_RATIO\n",
    "            \n",
    "        for i in range(int(data.shape[0] // (BATCH_SIZE * TRAINING_RATIO))):\n",
    "            discriminator_minibatches = data[\n",
    "                i * minibatches_size: (i + 1) * minibatches_size\n",
    "            ]\n",
    "            noise = np.random.normal(0, 1, [BATCH_SIZE * TRAINING_RATIO, 100])\n",
    "\n",
    "            for j in range(TRAINING_RATIO):\n",
    "                image_batch = discriminator_minibatches[\n",
    "                    j * BATCH_SIZE: (j + 1) * BATCH_SIZE\n",
    "                ]\n",
    "                noise_batch = noise[j * BATCH_SIZE: (j + 1) * BATCH_SIZE]\n",
    "\n",
    "                discriminator_loss.append(\n",
    "                    discriminator_model.train_on_batch(\n",
    "                        [image_batch, noise_batch], [positive_y, negative_y, dummy_y] # positive and negative?? check\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "            noise = np.random.normal(0, 1, [BATCH_SIZE, 100])\n",
    "            generator_loss.append(\n",
    "                generator_model.train_on_batch(noise, [positive_y, image_batch])\n",
    "            )\n",
    "        print(epoch, generator_loss)\n",
    "\n",
    "        # save every 1000 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch)\n",
    "            SAMPLE_SIZE = data.shape[0]\n",
    "            noise = np.random.normal(0, 1, [SAMPLE_SIZE, 100])\n",
    "            results = generator.predict(noise)\n",
    "            for i in range(4):    \n",
    "                plt.hist(results[:,i], histtype = 'step', bins = 100)\n",
    "                plt.hist(data[:,i], histtype = 'step', bins = 100)\n",
    "                plt.legend(['GAN', 'True'])\n",
    "                plt.show()\n",
    "#             generator.save_weights( IMAGE_DIR_PATH + \"generator\" + str(epoch // 100).zfill(5) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_FAT_GAN(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
